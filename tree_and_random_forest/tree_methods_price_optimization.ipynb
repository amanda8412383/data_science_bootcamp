{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690c766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f26a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e5c639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>flavour</th>\n",
       "      <th>volume_per_joghurt_g</th>\n",
       "      <th>packsize</th>\n",
       "      <th>product_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>units</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mueller</td>\n",
       "      <td>blueberry</td>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1013400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mueller</td>\n",
       "      <td>blueberry</td>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>4.76</td>\n",
       "      <td>1205.0</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>1084500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mueller</td>\n",
       "      <td>blueberry</td>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>5.22</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>Friday</td>\n",
       "      <td>906300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mueller</td>\n",
       "      <td>blueberry</td>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>5.45</td>\n",
       "      <td>1282.0</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1153800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mueller</td>\n",
       "      <td>blueberry</td>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>933300.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     brand    flavour  volume_per_joghurt_g  packsize  product_id  Unnamed: 0  \\\n",
       "0  Mueller  blueberry                   150         6           0           0   \n",
       "1  Mueller  blueberry                   150         6           0           1   \n",
       "2  Mueller  blueberry                   150         6           0           2   \n",
       "3  Mueller  blueberry                   150         6           0           3   \n",
       "4  Mueller  blueberry                   150         6           0           4   \n",
       "\n",
       "         date  price   units    weekday     weight  \n",
       "0  2020-01-01   4.65  1126.0  Wednesday  1013400.0  \n",
       "1  2020-01-02   4.76  1205.0   Thursday  1084500.0  \n",
       "2  2020-01-03   5.22  1007.0     Friday   906300.0  \n",
       "3  2020-01-04   5.45  1282.0   Saturday  1153800.0  \n",
       "4  2020-01-05   3.33  1037.0     Sunday   933300.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8c8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_forest():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reg = None\n",
    "\n",
    "    def pred(self, x):\n",
    "        if self.reg is None:\n",
    "            raise ValueError(\"train the model first\")\n",
    "        else:\n",
    "            self.reg.predict(x)\n",
    "        return self.reg.predict(x)\n",
    "\n",
    "    def random_forest_on_units(self, x):\n",
    "\n",
    "        #def y\n",
    "        y = df[['units']]\n",
    "\n",
    "\n",
    "        #skip standardization bcs it's unnecessary for tree based models\n",
    "        #test train split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        #fit model\n",
    "        self.reg  = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        self.reg.fit(X_train, y_train)\n",
    "\n",
    "        preds = self.reg.predict(X_test)\n",
    "\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        print(f\"MSE: {mse:.3f}\\n\")\n",
    "        print(f\"R^2: {r2:.3f}\\n\")\n",
    "        print(f\"accuracy: {self.reg.score(X_test, y_test):.3f}\\n\")\n",
    "        print(self.reg.feature_importances_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2371fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df[['price']]\n",
    "y = df[['units']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357adf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 135833.616\n",
      "\n",
      "R^2: 0.307\n",
      "\n",
      "accuracy: 0.307\n",
      "\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "rf_x1 = random_forest()\n",
    "rf_x1.random_forest_on_units(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d87048b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = df[['brand', 'flavour', 'volume_per_joghurt_g', 'packsize', 'price']]\n",
    "x3 = df[['brand', 'flavour',  'price']]\n",
    "\n",
    "dummy_df = pd.get_dummies(x3, columns=['brand', 'flavour'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009aaa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 41938.957\n",
      "\n",
      "R^2: 0.786\n",
      "\n",
      "accuracy: 0.786\n",
      "\n",
      "[0.75513195 0.02226854 0.0085416  0.00991372 0.00610091 0.00729324\n",
      " 0.03298408 0.01183795 0.01014182 0.05367989 0.00748972 0.06415384\n",
      " 0.01046274]\n"
     ]
    }
   ],
   "source": [
    "rf_xn = random_forest()\n",
    "rf_xn.random_forest_on_units(dummy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0d9349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class gradient_boosting():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reg = None\n",
    "\n",
    "    def pred(self, x):\n",
    "        if self.reg is None:\n",
    "            raise ValueError(\"train the model first\")\n",
    "        else:\n",
    "            self.reg.predict(x)\n",
    "        return self.reg.predict(x)\n",
    "\n",
    "    def gradient_boosting_on_units(self, x):\n",
    "\n",
    "        #def y\n",
    "        y = df[['units']]\n",
    "\n",
    "\n",
    "        #skip standardization bcs it's unnecessary for tree based models\n",
    "        #test train split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        #fit model\n",
    "        self.reg = GradientBoostingRegressor(n_estimators=500, max_depth=1, learning_rate=0.1, random_state=42)\n",
    "        self.reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        preds = self.reg.predict(X_test)\n",
    "\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        print(f\"MSE: {mse:.3f}\\n\")\n",
    "        print(f\"R^2: {r2:.3f}\\n\")\n",
    "        print(f\"accuracy: {self.reg.score(X_test, y_test):.3f}\\n\")\n",
    "        print(self.reg.feature_importances_)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ea7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 89226.922\n",
      "\n",
      "R^2: 0.545\n",
      "\n",
      "accuracy: 0.545\n",
      "\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "gb_x1 = gradient_boosting()\n",
    "gb_x1.gradient_boosting_on_units(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25fafb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 50035.875\n",
      "\n",
      "R^2: 0.745\n",
      "\n",
      "accuracy: 0.745\n",
      "\n",
      "[7.87636689e-01 1.74807082e-02 1.63466862e-03 0.00000000e+00\n",
      " 7.08572682e-04 0.00000000e+00 8.95358133e-04 0.00000000e+00\n",
      " 7.88564037e-03 8.95435333e-02 1.57521730e-02 7.84626569e-02\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "gb_xn = gradient_boosting()\n",
    "gb_xn.gradient_boosting_on_units(dummy_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb72da7",
   "metadata": {},
   "source": [
    "\n",
    "### tuning gradient boosting tree\n",
    "\n",
    "**step for optimization**\n",
    "1. Choose a relatively high learning rate(0.1 or 0.05 - 0.2)\n",
    "2. choose  number of trees 40-70 that the system can work fairly fast to test various scenarios and determine the tree parameters\n",
    "3. Tune tree-specific parameters\n",
    "4. Lower the learning rate and increase the estimators proportionally to get more robust models\n",
    "\n",
    "**parameter for managing boosting**\n",
    "1. **learning rate**\n",
    "    learning rate shrinks the contribution of each tree by learning_rate. high learning rate results in overfitting. low values always work better, given that we train on sufficient number of trees\n",
    "\n",
    "2. **N_estimators**\n",
    "    it's the number of trees in the forest. Usually the higher the number of trees the better to learn the data. However, adding a lot of trees can slow down the training process considerably\n",
    "\n",
    "3. **subsample** \n",
    "    The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "    Values slightly less than 1 make the model robust by reducing the variance.\n",
    "    subsample = 0.8 : This is a commonly used used start value\n",
    "\n",
    "\n",
    "**parameters used for defining a tree**\n",
    "\n",
    "1. **min_samples_split**\n",
    "    min number of samples  which are required in a node to be considered for splitting.\n",
    "    Higher values prevent a model from learning relations specific to the particular sample selected for a tree (over-fitting). Too high can lead to under-fitting \n",
    "\n",
    "    initial value should be ~0.5-1% of total values. if there is a imbalanced class problem, small value from the range should be taken. under assumption of full tree and equal split: min_samples_split ≥ N / (2^max_depth)\n",
    "\n",
    "2. **min_samples_leaf**\n",
    "    min samples required in a terminal node. Used to control over-fitting similar to min_samples_split. lower values should be chosen for imbalanced class problems. Can be selected based on intuition.\n",
    "    \n",
    "    Similar to min_weight_fraction_leaf which defined as a fraction of the total number of observations instead of an integer.\n",
    "    Only one of them should be defined.\n",
    "    under assumption of full tree and equal split: N / (2^max_depth) ≥ min_samples_leaf\n",
    "\n",
    "    \n",
    "\n",
    "3. **max_depth**\n",
    "    The maximum depth of a tree.\n",
    "    Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample. Should be chosen (5-8) based on the number of observations and predictors. E.g. 87K rows and 49 columns can set depth at 8\n",
    "\n",
    "    if max_leaf_nodes is defined, GBM will ignore max_depth, which is The maximum number of terminal nodes or leaves in a tree, since a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "\n",
    "4. **max_features**\n",
    "    The number of features to consider while searching for a best split. These will be randomly selected. \n",
    "\n",
    "    As a thumb-rule, square root of the total number of features works great and used as start value. but we should check upto 30-40% of the total number of features. Higher values can lead to over-fitting.\n",
    "\n",
    "**miscellaneous parameters**\n",
    "\n",
    "1. loss = loss function to be minimized in each split.Generally the default values work fine. \n",
    "\n",
    "2. init = initialization of the output. This can be used if we have made another model whose outcome is to be used as the initial estimates for GBM.\n",
    "3. random_state. for parameter tuning if it's not fixed, then we’ll have different outcomes for subsequent runs on the same parameters and it becomes difficult to compare models. It can potentially result in overfitting to a particular random sample selected. But running models for different random samples is computationally expensive and generally not used.\n",
    "4. verbose = The type of output to be printed\n",
    "5. warm_start = capability to fit additional trees on previous fits of a model. It can save a lot of time and should be explored for advanced applications\n",
    "6. presort = whether to presort data for faster splits. It makes the selection automatically by default but it can be changed if needed.\n",
    "\n",
    "source: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "**parameter for GridSearchCV**\n",
    "\n",
    "1. cv: Determines the cross-validation splitting strategy. Possible inputs :\n",
    "\n",
    "    - None, to use the default 5-fold cross validation,\n",
    "\n",
    "    - integer, to specify the number of folds in a (Stratified)KFold,\n",
    "\n",
    "    - CV splitter, An iterable yielding (train, test) splits as arrays of indices.\n",
    "2. n_jobs = Number of jobs to run in parallel\n",
    "3. iid = \n",
    "4. scoring = Strategy to evaluate the performance of the cross-validated model on the test set.\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40ec8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dummy_df, df[['units']], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "98f40f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50} 0.591324712824672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.792533320820445"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tuning n_estimators\n",
    "param_test1 = {'n_estimators':range(10,101,10)}\n",
    "gsearch1 = GridSearchCV(estimator = \n",
    "                        GradientBoostingRegressor(learning_rate=0.05,\n",
    "                                                    min_samples_leaf=int(dummy_df.shape[0]/100),\n",
    "                                                    max_depth=7,\n",
    "                                                    max_features='sqrt',\n",
    "                                                    subsample=0.8,\n",
    "                                                    random_state=42), \n",
    "param_grid = param_test1, scoring='r2',n_jobs=4, cv=2)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "#gsearch1.cv_results_, \n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "gsearch1.best_estimator_.fit(X_train, y_train)\n",
    "gsearch1.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc09d7",
   "metadata": {},
   "source": [
    "- If the value is around 60, it's reasonable for learning rate 0.1\n",
    "- If the value is around 20, you might want to try lowering the learning rate to 0.05 and re-run grid search\n",
    "- If the values are too high ~100, tuning the other parameters will take long time and you can try a higher learning rate\n",
    "\n",
    "### Tuning tree-specific parameters\n",
    "\n",
    "The order of tuning variables should be decided carefully. variables with a higher impact on outcome first. For instance, max_depth and min_samples_split have a significant impact.\n",
    "\n",
    "To start with, we’ll test max_depth values and min_samples_split based on intuition. Setting a wider ranges then perform multiple iterations for smaller ranges is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "33b533ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'min_samples_leaf': 5} 0.6118561125488826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7506086302156465"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {'max_depth':range(1,7,1), 'min_samples_leaf':range(1,20,1)}\n",
    "#tuning max_depth\n",
    "gsearch2 = GridSearchCV(estimator = \n",
    "                        GradientBoostingRegressor(learning_rate=0.05,\n",
    "                                                    n_estimators=50,\n",
    "                                                    max_features='sqrt',\n",
    "                                                    subsample=0.8,\n",
    "                                                    random_state=42), \n",
    "param_grid = param_test2, scoring='r2',n_jobs=4, cv=2)\n",
    "gsearch2.fit(X_train, y_train)\n",
    "#gsearch1.cv_results_, \n",
    "print(gsearch2.best_params_, gsearch2.best_score_)\n",
    "gsearch2.best_estimator_.fit(X_train, y_train)\n",
    "gsearch2.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ed360",
   "metadata": {},
   "source": [
    "(7, 2) have better performance than (5,5) on test data, but for training data (5,5) fits better, and therefore we see a decrese in performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "763d1217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 2, 'min_samples_split': 17} 0.6163155304121801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "10 fits failed out of a total of 190.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of GradientBoostingRegressor must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.58854042 0.59582788 0.59960842 0.59864796 0.59686714\n",
      " 0.59990052 0.60264135 0.60268676 0.61056077 0.61073489 0.61031186\n",
      " 0.61095086 0.6118716  0.60758529 0.61499354 0.6137119  0.60947397\n",
      " 0.60696502        nan 0.60248207 0.60248207 0.60248207 0.59767221\n",
      " 0.60201576 0.60474557 0.60681106 0.60556465 0.61470135 0.60966748\n",
      " 0.61017228 0.61142766 0.61220945 0.61222365 0.61547925 0.61631553\n",
      " 0.61454082 0.6149545         nan 0.60462268 0.60462268 0.60462268\n",
      " 0.60462268 0.60462268 0.60710655 0.60628243 0.60702029 0.61224341\n",
      " 0.60522745 0.60459227 0.60912836 0.6140464  0.60228658 0.61006929\n",
      " 0.61336281 0.61388005 0.61135048        nan 0.60462965 0.60462965\n",
      " 0.60462965 0.60462965 0.60462965 0.60462965 0.60462965 0.61011217\n",
      " 0.60915666 0.61318359 0.60758783 0.60839385 0.60778269 0.60667807\n",
      " 0.60924221 0.60939183 0.60838217 0.60989264        nan 0.61185611\n",
      " 0.61185611 0.61185611 0.61185611 0.61185611 0.61185611 0.61185611\n",
      " 0.61185611 0.61185611 0.61328578 0.61221637 0.60990922 0.60886034\n",
      " 0.6077671  0.61072397 0.61330496 0.61037498 0.60973689]\n",
      "  warnings.warn(\n",
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7808800724202684"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {'min_samples_split':range(1,20,1), 'min_samples_leaf':range(1,6,1)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.05, \n",
    "                                                                n_estimators=50,\n",
    "                                                                max_depth=5,\n",
    "                                                                max_features='sqrt', \n",
    "                                                                subsample=0.8, \n",
    "                                                                random_state=42), \n",
    "param_grid = param_test3, scoring='r2',n_jobs=4, cv=2)\n",
    "gsearch3.fit(X_train, y_train)\n",
    "#gsearch1.cv_results_, \n",
    "print(gsearch3.best_params_, gsearch3.best_score_)\n",
    "gsearch3.best_estimator_.fit(X_train, y_train)\n",
    "gsearch3.best_estimator_.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5277cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 3} 0.6163155304121801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7808800724202684"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tuning max_features\n",
    "param_test4 = {'max_features':range(1,13,1)}\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.05, \n",
    "                                                                n_estimators=50,\n",
    "                                                                max_depth=5,\n",
    "                                                                subsample=0.8, \n",
    "                                                                min_samples_split=17,\n",
    "                                                                min_samples_leaf=2,\n",
    "                                                                random_state=42), \n",
    "param_grid = param_test4, scoring='r2',n_jobs=4, cv=2)\n",
    "gsearch4.fit(X_train, y_train)\n",
    "#gsearch1.cv_results_, \n",
    "print(gsearch4.best_params_, gsearch4.best_score_)\n",
    "gsearch4.best_estimator_.fit(X_train, y_train)\n",
    "gsearch4.best_estimator_.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06ab6d",
   "metadata": {},
   "source": [
    "3 is same as square method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cebb1d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.75} 0.583462361405857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n",
      "c:\\Users\\test\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:672: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7972034632926046"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tuning max_features\n",
    "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.05, \n",
    "                                                                n_estimators=50,\n",
    "                                                                max_depth=5,\n",
    "                                                                min_samples_split=17,\n",
    "                                                                min_samples_leaf=2,\n",
    "                                                                random_state=42), \n",
    "param_grid = param_test5, scoring='r2',n_jobs=4, cv=2)\n",
    "gsearch5.fit(X_train, y_train)\n",
    "#gsearch1.cv_results_, \n",
    "print(gsearch5.best_params_, gsearch5.best_score_)\n",
    "gsearch5.best_estimator_.fit(X_train, y_train)\n",
    "gsearch5.best_estimator_.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ce660",
   "metadata": {},
   "source": [
    "# model comparison for this dataset\n",
    "\n",
    "### OLS\n",
    "\n",
    "- linearity v\n",
    "- endogeneity: can only assume Omitted variables/Simultaneity/Measurement error/ Sample selection bias doesn't exist\n",
    "- normality and homoscedasticity of residual v\n",
    "- autocorrelation: if the data are collected from different customers then it's safe otherwise this would lead to underestimate of std\n",
    "- multicolinearity: price has a high VIF, which leads to inflated standard errors and biased estimation. since we are mainly looking at the effect of price affecting sales, 'volume_per_joghurt_g' and 'packsize' are dropped to increase the price precission even though it decrease adj r squared\n",
    "\n",
    "\n",
    "\n",
    "### model: random forest\n",
    "\n",
    "- n_estimators= 100 is the default value to balance of performance and speed A\n",
    "- Independence of Observations: if the data is not collected from different customer this may lead to overfitting or misleading variable importance. Tree-based methods do not inherently account for autocorrelation... Time-series structure must be explicitly encoded via lag features or temporal validation\n",
    "- minimal multicollinearity: may split on redundant variables/ misleading importance rankings. Random Forest is not affected by multicollinearity as much as linear regression... though variable importance can be biased.\n",
    "\n",
    "- however the above 2 problems also affect linear model, and random forest is more robust to them in general. \n",
    "\n",
    "### comparing random forest and OLS\n",
    "\n",
    "- Cross-validation and scoring metrics like MSE or R² can be used to compare the generalization performance of multiple estimators.\n",
    "- Adjusted R² is not well defined for non-parametric models like Random Forests.\n",
    "- Standardizing the target variable allows MSE to be interpreted in terms of variance units, but it is no longer in the original units of measurement, making direct interpretability more difficult.\n",
    "- R² depends only on the squared errors and the total variance, and both scale in the same way under standardization, R² remains unchanged\n",
    "\n",
    "### other alternatives\n",
    "\n",
    "- **gradient boosting tree regresssion :**\n",
    "\n",
    "    potentoal of higher accuracy than random forest, but requires more tuning and is more prone to overfitting\n",
    "\n",
    "- **GLM :** generalizes OLS by allowing for non-normal dependent variables (e.g., logistic regression, Poisson regression). It is best when the data type itself is different—like counts or proportions.\n",
    "    - Linearity \n",
    "    - No Endogeneity \n",
    "    - No Multicollinearity \n",
    "    - No Autocorrelation\n",
    "\n",
    "    specific to GLM compared to OLS:\n",
    "    - Correct Specification of the Link Functio\n",
    "    - Correct Specification of the Distribution of the Response Variable (exponential family e.g., Normal, Binomial, Poisson, Gamma, Inverse Gaussian)\n",
    "    - Some GLMs require handling of an extra dispersion parameter (e.g., in Gamma or quasi-Poisson models).\n",
    "\n",
    "    difference from OLS, GLM can better handle the below assumptions, but we don't have to worry about them here:\n",
    "    - Normality and Homoscedasticity of Residuals\n",
    "        - Normality: GLM assumes that the response variable follows a distribution from the exponential family\n",
    "        - Homoscedasticity : GLM allows the variance to be a function of the mean, depending on the family \n",
    "\n",
    "- **GLS :**  generalizes OLS by relaxing the assumption of homoscedasticity and uncorrelated errors. It is used when the dependent variable is continuous and normally distributed, but errors are problematic (e.g., heteroscedastic or autocorrelated)   \n",
    "    - Linearity \n",
    "    - No Endogeneity \n",
    "    - No Multicollinearity \n",
    "\n",
    "    specific to GLS compared to OLS:\n",
    "    - Known or Correctly Specified Error Variance-Covariance Matrix (Ω)\n",
    "        - use GLS If Ω is known\n",
    "        - use Feasible GLS if Ω must be estimated\n",
    "\n",
    "    difference from OLS, GLS can better handle the below assumptions, but we don't have to worry about them here:\n",
    "    - Normality and Homoscedasticity of Residuals\n",
    "        - Normality: Not necessary for unbiasedness/consistency, only for inference\n",
    "        - Homoscedasticity : GLS is used precisely when heteroscedasticity is present\n",
    "    - No Autocorrelation : GLS is often employed specifically to address autocorrelation \n",
    "\n",
    "- **GAM :** Generalized Additive Models \n",
    "    - No Endogeneity \n",
    "    - No Multicollinearity \n",
    "    - No Autocorrelation\t\n",
    "\n",
    "    specific to GAM compared to OLS:\n",
    "    - Smoothness Selection & Overfitting Control: GAMs require selecting the degree of smoothness for each function (e.g., via penalized likelihood)\n",
    "    - Basis Function Specification: GAM is sensitive to the choice and number of basis functions used to represent smooth terms.\n",
    "    - Additivity: GAM assumes that effects of predictors are additive, unless interactions are explicitly modeled.\n",
    "\n",
    "\n",
    "\n",
    "    difference from OLS, GAM can better handle the below assumptions:\n",
    "     - Linearity: Predictors are allowed to have non-linear relationships with the response via smooth functions (e.g., splines). \n",
    "    - Normality and Homoscedasticity of Residuals\n",
    "        - Normality: Required only if using Gaussian family, not required for all GAMs (GAMs can use Poisson, binomial, etc.)\n",
    "        - Homoscedasticity: Assumes variance is correctly modeled via the distribution in the specified family; for Gaussian GAMs, this implies constant variance unless otherwise modeled\n",
    "\n",
    "    here maybe GAM can fit a better line than OLS, but the difference can't be too drastic based on the graph\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
