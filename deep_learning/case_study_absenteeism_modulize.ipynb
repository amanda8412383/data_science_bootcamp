{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b7e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\anaconda3\\envs\\bike\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997ae49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd7c1c8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- higher_education\n- reason_group_accident\n- reason_group_other\n- reason_group_pregnancy\n- reason_group_sickness\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13696/1844921694.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[0mAbsent_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbsent_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model.sav\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"scaler.sav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mAbsent_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_and_clean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Absenteeism_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13696/1844921694.py\u001b[0m in \u001b[0;36mload_and_clean_data\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessed_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdata_existence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\test\\anaconda3\\envs\\bike\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\test\\anaconda3\\envs\\bike\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m         X = validate_data(\n\u001b[0m\u001b[0;32m   1063\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\test\\anaconda3\\envs\\bike\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2917\u001b[0m         \u001b[0mvalidated\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2918\u001b[0m     \"\"\"\n\u001b[1;32m-> 2919\u001b[1;33m     \u001b[0m_check_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2920\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2921\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\test\\anaconda3\\envs\\bike\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2775\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Feature names must be in the same order as they were in fit.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- higher_education\n- reason_group_accident\n- reason_group_other\n- reason_group_pregnancy\n- reason_group_sickness\n- ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    \n",
    "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
    "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
    "        self.columns = columns\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        print(X.columns)\n",
    "        print(self.scaler.columns)\n",
    "        init_col_order = X.columns\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]\n",
    "    \n",
    "class Absent_model():\n",
    "\n",
    "    def __init__(self, model_file, scaler_file):\n",
    "        self.reg = pickle.load(open(model_file, 'rb'))\n",
    "        self.scaler = pickle.load(open(scaler_file, 'rb'))\n",
    "        self.data = None\n",
    "    \n",
    "\n",
    "    def load_and_clean_data(self, file_name):\n",
    "        raw = pd.read_csv(file_name,delimiter=',')\n",
    "\n",
    "        raw[\"reason_group\"] = pd.cut(\n",
    "            x=raw[\"Reason for Absence\"],\n",
    "            bins=[-np.inf, 0, 14, 17, 21, np.inf],\n",
    "            labels=[\"unknown\", \"sickness\", \"pregnancy\", \"accident\", \"other\"],\n",
    "\n",
    "        )\n",
    "        \n",
    "        reason_dummy = pd.get_dummies(raw[\"reason_group\"], prefix=\"reason_group\", drop_first=True)\n",
    "        df = pd.concat([raw, reason_dummy], axis=1)\n",
    "        df['timestamp'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['day_of_week'] = df['timestamp'].dt.day_of_week\n",
    "        df['higher_education'] = df['Education'].apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "        #target Einstufung\n",
    "        # der vorteil, der median zu benutyen ist, dass die Daten ausgleich geteilt werden \n",
    "        benchmark = df['Absenteeism Time in Hours'].median()\n",
    "        df['target'] = df['Absenteeism Time in Hours'].apply(lambda x: 1 if x > benchmark else 0)\n",
    "        #spalten die wir nicht brauchen\n",
    "        df.drop(columns=['Date', 'ID', 'Reason for Absence', 'Absenteeism Time in Hours', 'reason_group', 'timestamp', 'Education'], inplace=True)\n",
    "        #Rückwärtseliminierung\n",
    "        df.drop(columns=['month', 'Distance to Work', 'Daily Work Load Average'], inplace=True)\n",
    "        \n",
    "        \n",
    "        self.preprocessed_data = df\n",
    "\n",
    "        self.data = self.scaler.transform(df)\n",
    "\n",
    "    def data_existence(self):\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Data not loaded. Please load data first.\")\n",
    "        else: \n",
    "            print(\"Data loaded successfully.\")\n",
    "            return True\n",
    "\n",
    "    def predict_probability(self):\n",
    "        if self.data_existence():\n",
    "            pred = self.reg.predict_proba(self.data)[:,1]\n",
    "            return pred\n",
    "\n",
    "    def predict_output(self):\n",
    "        if self.data_existence():\n",
    "            pred = self.reg.predict(self.data)[:,1]\n",
    "            return pred\n",
    "        \n",
    "    def adding_predicted_to_df(self):\n",
    "        if self.data_existence():\n",
    "            self.data['Probability'] = self.predict_probability()\n",
    "            self.data ['Prediction'] = self.predict_output()\n",
    "            return self.data       \n",
    "\n",
    "Absent_model = Absent_model(\"model.sav\", \"scaler.sav\")\n",
    "Absent_model.load_and_clean_data('Absenteeism_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a777f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Absent_model.predict_probability()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e940628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stadardize_data(df, column_exclude):\n",
    "\n",
    "input = df.iloc[:, :-1]\n",
    "\n",
    "dummy_column =['reason_group_sickness', 'reason_group_pregnancy',\n",
    "       'reason_group_accident', 'reason_group_other', 'higher_education']\n",
    "column_scaled = [x for x in input.columns if x not in dummy_column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbdfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#die Daten satnardisieren\n",
    "#dummy Spalten sollte  nicht skaliert werden, weil sie nicht interpretiert werden können\n",
    "#normalerweise werden die Daten skaliert, vor dummy erstellung\n",
    "scaler = StandardScaler()\n",
    "\n",
    "input[column_scaled] = scaler.fit_transform(input[column_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ad6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#daten in train und test aufteilen und mischen\n",
    "#standardmäßig schuffle = True\n",
    "#stratify = True, damit die Verteilung der Zielvariable gleich bleibt\n",
    "#stratofy link https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, df_select['target'], test_size=0.2, random_state=42, stratify=df_select['target'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8af719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7642857142857142, 0.7642857142857142)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(x_train, y_train)\n",
    "logreg.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81b879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642857142857142"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#überprüfung der Vorhersage\n",
    "y_pred = logreg.predict(x_train)\n",
    "np.sum(y_pred == y_train) / y_train.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#erstellen der Confusion tabelle\n",
    "feature_name = input.columns.values\n",
    "summary = pd.DataFrame(data=feature_name, columns=[\"feature\"])\n",
    "summary['coefficient'] = logreg.coef_.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Achsenabschnitt hinzufügen\n",
    "summary.loc[-1] = ['intercept', logreg.intercept_[0]]  # adding a row\n",
    "summary.index = summary.index + 1  # shifting index\n",
    "summary = summary.sort_index()  # sorting by index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33489b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Odds Ratio hinzufügen\n",
    "#wenn die Wahrscheinlichkeit 5/1 und die Odds Ratio 2 ist, für eine Einheitsänderung des Eingabe steigen die Wahrscheinlichkeit 2*5/1\n",
    "summary['odds_ratio'] = np.exp(summary['coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02169d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reason_group_accident</td>\n",
       "      <td>3.130692</td>\n",
       "      <td>22.889822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reason_group_sickness</td>\n",
       "      <td>2.765370</td>\n",
       "      <td>15.884919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reason_group_other</td>\n",
       "      <td>0.953105</td>\n",
       "      <td>2.593752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reason_group_pregnancy</td>\n",
       "      <td>0.763485</td>\n",
       "      <td>2.145741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation Expense</td>\n",
       "      <td>0.597400</td>\n",
       "      <td>1.817387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children</td>\n",
       "      <td>0.465434</td>\n",
       "      <td>1.592705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Body Mass Index</td>\n",
       "      <td>0.294789</td>\n",
       "      <td>1.342843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>higher_education</td>\n",
       "      <td>0.202941</td>\n",
       "      <td>1.225001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age</td>\n",
       "      <td>-0.154169</td>\n",
       "      <td>0.857127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>-0.246102</td>\n",
       "      <td>0.781842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pets</td>\n",
       "      <td>-0.308507</td>\n",
       "      <td>0.734543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept</td>\n",
       "      <td>-1.728339</td>\n",
       "      <td>0.177579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature  coefficient  odds_ratio\n",
       "8    reason_group_accident     3.130692   22.889822\n",
       "6    reason_group_sickness     2.765370   15.884919\n",
       "9       reason_group_other     0.953105    2.593752\n",
       "7   reason_group_pregnancy     0.763485    2.145741\n",
       "1   Transportation Expense     0.597400    1.817387\n",
       "4                 Children     0.465434    1.592705\n",
       "3          Body Mass Index     0.294789    1.342843\n",
       "11        higher_education     0.202941    1.225001\n",
       "2                      Age    -0.154169    0.857127\n",
       "10             day_of_week    -0.246102    0.781842\n",
       "5                     Pets    -0.308507    0.734543\n",
       "0                intercept    -1.728339    0.177579"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sort_values(by='odds_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a39907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642857142857142"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model evaluieren\n",
    "logreg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#die Wahrscheinlichkeit die Mitarbeiterin abwesent ist\n",
    "y_pred_proba = logreg.predict_proba(x_test)[:, 1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
