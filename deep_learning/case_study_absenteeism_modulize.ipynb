{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b7e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\anaconda3\\envs\\bike\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d997ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Absent_model():\n",
    "\n",
    "    def __init__(self, model_file, scaler_file):\n",
    "        self.reg = pickle.load(open(model_file, 'rb'))\n",
    "        self.scaler_file = pickle.load(open(scaler_file, 'rb'))\n",
    "        self.data = None\n",
    "    \n",
    "    def load_and_clean_data(self, file_name):\n",
    "        raw = pd.read_csv(file_name,delimiter=',')\n",
    "\n",
    "        raw[\"reason_group\"] = pd.cut(\n",
    "            x=raw[\"Reason for Absence\"],\n",
    "            bins=[-np.inf, 0, 14, 17, 21, np.inf],\n",
    "            labels=[\"unknown\", \"sickness\", \"pregnancy\", \"accident\", \"other\"],\n",
    "\n",
    "        )\n",
    "        \n",
    "        reason_dummy = pd.get_dummies(raw[\"reason_group\"], prefix=\"reason_group\", drop_first=True)\n",
    "        df = pd.concat([raw, reason_dummy], axis=1)\n",
    "        df['timestamp'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['day_of_week'] = df['timestamp'].dt.day_of_week\n",
    "        df['higher_education'] = df['Education'].apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "        #target Einstufung\n",
    "        # der vorteil, der median zu benutyen ist, dass die Daten ausgleich geteilt werden \n",
    "        benchmark = df['Absenteeism Time in Hours'].median()\n",
    "        df['target'] = df['Absenteeism Time in Hours'].apply(lambda x: 1 if x > benchmark else 0)\n",
    "        #spalten die wir nicht brauchen\n",
    "        df.drop(columns=['Date', 'ID', 'Reason for Absence', 'Absenteeism Time in Hours', 'reason_group', 'timestamp', 'Education'], inplace=True)\n",
    "        #Rückwärtseliminierung\n",
    "        df.drop(columns=['month', 'Distance to Work', 'Daily Work Load Average'], inplace=True)\n",
    "        \n",
    "        self.preprocessed_data = df\n",
    "        #return df\n",
    "        #self.data = self.scaler.transform(df)\n",
    "\n",
    "Absent_model = Absent_model(\"model.sav\", \"scaler.sav\")\n",
    "Absent_model.load_and_clean_data('Absenteeism_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7c1c8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_and_preprocess_data() missing 1 required positional argument: 'file_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13696/3748522281.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Absenteeism_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: load_and_preprocess_data() missing 1 required positional argument: 'file_name'"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(self, file_name):\n",
    "    raw = pd.read_csv(file_name,delimiter=',')\n",
    "\n",
    "    raw[\"reason_group\"] = pd.cut(\n",
    "        x=raw[\"Reason for Absence\"],\n",
    "        bins=[-np.inf, 0, 14, 17, 21, np.inf],\n",
    "        labels=[\"unknown\", \"sickness\", \"pregnancy\", \"accident\", \"other\"],\n",
    "\n",
    "    )\n",
    "    \n",
    "    reason_dummy = pd.get_dummies(raw[\"reason_group\"], prefix=\"reason_group\", drop_first=True)\n",
    "    df = pd.concat([raw, reason_dummy], axis=1)\n",
    "    df['timestamp'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day_of_week'] = df['timestamp'].dt.day_of_week\n",
    "    df['higher_education'] = df['Education'].apply(lambda x: 1 if x > 1 else 0)\n",
    "\n",
    "    #target Einstufung\n",
    "    # der vorteil, der median zu benutyen ist, dass die Daten ausgleich geteilt werden \n",
    "    benchmark = df['Absenteeism Time in Hours'].median()\n",
    "    df['target'] = df['Absenteeism Time in Hours'].apply(lambda x: 1 if x > benchmark else 0)\n",
    "    #spalten die wir nicht brauchen\n",
    "    df.drop(columns=['Date', 'ID', 'Reason for Absence', 'Absenteeism Time in Hours', 'reason_group', 'timestamp', 'Education'], inplace=True)\n",
    "    #Rückwärtseliminierung\n",
    "    df.drop(columns=['month', 'Distance to Work', 'Daily Work Load Average'], inplace=True)\n",
    "    \n",
    "    self.preprocessed_data = df\n",
    "    #return df\n",
    "    #self.data = self.scaler.transform(df)\n",
    "\n",
    "\n",
    "df = load_and_preprocess_data(\"Absenteeism_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e940628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stadardize_data(df, column_exclude):\n",
    "\n",
    "input = df.iloc[:, :-1]\n",
    "\n",
    "dummy_column =['reason_group_sickness', 'reason_group_pregnancy',\n",
    "       'reason_group_accident', 'reason_group_other', 'higher_education']\n",
    "column_scaled = [x for x in input.columns if x not in dummy_column]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbdfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#die Daten satnardisieren\n",
    "#dummy Spalten sollte  nicht skaliert werden, weil sie nicht interpretiert werden können\n",
    "#normalerweise werden die Daten skaliert, vor dummy erstellung\n",
    "scaler = StandardScaler()\n",
    "\n",
    "input[column_scaled] = scaler.fit_transform(input[column_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64ad6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#daten in train und test aufteilen und mischen\n",
    "#standardmäßig schuffle = True\n",
    "#stratify = True, damit die Verteilung der Zielvariable gleich bleibt\n",
    "#stratofy link https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, df_select['target'], test_size=0.2, random_state=42, stratify=df_select['target'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b81df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8af719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7642857142857142, 0.7642857142857142)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(x_train, y_train)\n",
    "logreg.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f81b879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642857142857142"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#überprüfung der Vorhersage\n",
    "y_pred = logreg.predict(x_train)\n",
    "np.sum(y_pred == y_train) / y_train.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3d4f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#erstellen der Confusion tabelle\n",
    "feature_name = input.columns.values\n",
    "summary = pd.DataFrame(data=feature_name, columns=[\"feature\"])\n",
    "summary['coefficient'] = logreg.coef_.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b1f4d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Achsenabschnitt hinzufügen\n",
    "summary.loc[-1] = ['intercept', logreg.intercept_[0]]  # adding a row\n",
    "summary.index = summary.index + 1  # shifting index\n",
    "summary = summary.sort_index()  # sorting by index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33489b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Odds Ratio hinzufügen\n",
    "#wenn die Wahrscheinlichkeit 5/1 und die Odds Ratio 2 ist, für eine Einheitsänderung des Eingabe steigen die Wahrscheinlichkeit 2*5/1\n",
    "summary['odds_ratio'] = np.exp(summary['coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d02169d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reason_group_accident</td>\n",
       "      <td>3.130692</td>\n",
       "      <td>22.889822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reason_group_sickness</td>\n",
       "      <td>2.765370</td>\n",
       "      <td>15.884919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>reason_group_other</td>\n",
       "      <td>0.953105</td>\n",
       "      <td>2.593752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reason_group_pregnancy</td>\n",
       "      <td>0.763485</td>\n",
       "      <td>2.145741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation Expense</td>\n",
       "      <td>0.597400</td>\n",
       "      <td>1.817387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children</td>\n",
       "      <td>0.465434</td>\n",
       "      <td>1.592705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Body Mass Index</td>\n",
       "      <td>0.294789</td>\n",
       "      <td>1.342843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>higher_education</td>\n",
       "      <td>0.202941</td>\n",
       "      <td>1.225001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age</td>\n",
       "      <td>-0.154169</td>\n",
       "      <td>0.857127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>-0.246102</td>\n",
       "      <td>0.781842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pets</td>\n",
       "      <td>-0.308507</td>\n",
       "      <td>0.734543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intercept</td>\n",
       "      <td>-1.728339</td>\n",
       "      <td>0.177579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature  coefficient  odds_ratio\n",
       "8    reason_group_accident     3.130692   22.889822\n",
       "6    reason_group_sickness     2.765370   15.884919\n",
       "9       reason_group_other     0.953105    2.593752\n",
       "7   reason_group_pregnancy     0.763485    2.145741\n",
       "1   Transportation Expense     0.597400    1.817387\n",
       "4                 Children     0.465434    1.592705\n",
       "3          Body Mass Index     0.294789    1.342843\n",
       "11        higher_education     0.202941    1.225001\n",
       "2                      Age    -0.154169    0.857127\n",
       "10             day_of_week    -0.246102    0.781842\n",
       "5                     Pets    -0.308507    0.734543\n",
       "0                intercept    -1.728339    0.177579"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sort_values(by='odds_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b23e04c",
   "metadata": {},
   "source": [
    "### interpretation\n",
    "- Wenn jmd in reason_group_accident ist, ist er 22 mal wahrscheinlicher abwesend als jmd in reason_group_default.\n",
    "- Wenn Transportation Expense um eine standardisierte Einheit steigt, ist die Absentismuswahrscheinlichkeit 1,78 mal so hoch wie im Basismodell.\n",
    "- Wenn Age um eine standardisierte Einheit steigt, ist die Absentismuswahrscheinlichkeit 0,86 mal  geringer als im Basismodell.\n",
    "\n",
    "\n",
    "wenn die Eingabe standarlisiert ist, ist es schwer, die Bedeutung zu interpretieren\n",
    "\n",
    "wenn die Eingabe nicht standarlisiert ist, sinken die Genaurigkeit\n",
    "\n",
    "am Besten bereitet beide Model vor\n",
    "\n",
    "interpretieren logistic regression Koeffizienten https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/\n",
    "\n",
    "### Backward Elimination\n",
    "Rückwärtseliminierung ist eine Methode der Variablenauswahl, bei der man mit einem Modell startet, das alle Variablen enthält, und dann nacheinander die am wenigsten signifikanten Variablen entfernt. Dieses Verfahren wird oft in der Regression verwendet, um das Modell zu vereinfachen und zu verbessern. \n",
    "Normalerweise wird die Variable mit dem höchsten p-Wert (oder der geringsten Signifikanz) eliminiert, wenn der p-Wert über einem bestimmten Schwellenwert (z. B. 0,05) liegt. Aber hier fehlt uns der p-Wert, weil in ML sie glauben, dass ein ausreichend kleines Gewicht vernachlässigbar ist.\n",
    "\n",
    "- month\t\t1.079030\n",
    "- Distance to Work\t\t1.024061\n",
    "- Daily Work Load Average 0.951270\n",
    "\n",
    "hat odds_ratio fast gleich wie 1, deshalb entfernen wir diese Werte, und die Genaurigkeit hat nicht geändert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05a39907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7642857142857142"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model evaluieren\n",
    "logreg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e49d9a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#die Wahrscheinlichkeit die Mitarbeiterin abwesent ist\n",
    "y_pred_proba = logreg.predict_proba(x_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model speichern\n",
    "filename = 'logreg_model.sav'\n",
    "pickle.dump(logreg, open(filename, 'wb'))\n",
    "filename = 'scaler.sav'\n",
    "pickle.dump(scaler, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41457bf7",
   "metadata": {},
   "source": [
    "### Model Speichern Methoden\n",
    "1. Joblib von Scipy\n",
    "2. Json\n",
    "3. pickle\n",
    "\n",
    "potenzial Probleme mit pickle:\n",
    "\n",
    "1. Benutzung von unterschiedliche Python Version für pickling und unpicking kann Problemen verursachen\n",
    "2. pickle ist langsam für große Modell\n",
    "3. pickle ist nicht sicher \"Entpacken Sie niemals Daten, die Sie von einer nicht vertrauenswürdigen oder nicht authentifizierten Quelle erhalten haben.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
