{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc34d1c",
   "metadata": {},
   "source": [
    "### Schritt 1: Daten Vorbereiten\n",
    "\n",
    "\n",
    "1. Das Datensatz ausgleichen. Die Daten sind unbalanciert, was bedeutet, dass es viel mehr Beispiele für eine Klasse gibt als für die andere. Dies kann zu einem Modell führen, das nicht gut generalisiert. Wir können die Daten ausgleichen, indem wir die Anzahl der Beispiele in jeder Klasse anpassen. In diesem Fall verwenden wir die `RandomOverSampler`-Klasse von `imblearn` um die Daten auszugleichen.\n",
    "2. Die Daten normalisieren. Wir verwenden die `StandardScaler`-Klasse von `sklearn` um die Daten zu normalisieren. Dies ist wichtig, da einige Algorithmen empfindlich auf die Skala der Daten reagieren.\n",
    "3. Die Daten in Trainings- und Testdaten aufteilen. Wir verwenden die `train_test_split`-Funktion von `sklearn` um die Daten in Trainings- und Testdaten aufzuteilen. Dies ist wichtig, um sicherzustellen, dass das Modell auf neuen, unsichtbaren Daten getestet wird.\n",
    "4. Die Daten in Tensoren umwandeln. Wir verwenden die `torch.tensor`-Funktion von `torch` um die Daten in Tensoren umzuwandeln. Dies ist wichtig, da PyTorch mit Tensoren arbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a57a0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "120b4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#die Daten einlesen\n",
    "csv = np.loadtxt(\"Audiobooks_data.csv\", delimiter=\",\")\n",
    "input = csv[:, 1:-1] #Spalte 0 ID und letzte Spalte target entfernen\n",
    "target = csv[:, -1] #letzte Spalte target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a2046dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten mischen\n",
    "shuffled_indices = np.arange(input.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "shuffled_input = input[shuffled_indices]\n",
    "shuffled_target = target[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bccd39d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted macht 15.88327179778472% der Daten aus\n"
     ]
    }
   ],
   "source": [
    "#Target Verteilung checken\n",
    "target_1 = np.sum(shuffled_target)\n",
    "print(f\"converted macht {target_1/shuffled_target.shape[0]*100}% der Daten aus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten ausgleichen durch entfernen von 0er Werten\n",
    "#wenn wir diese Schritt nicht machen, haben wir die genaurigkeit von 80 bis 92% erhoht \n",
    "zero_target_counter = 0\n",
    "indices_to_remove = []\n",
    "for i in range(shuffled_target.shape[0]):\n",
    "    if shuffled_target[i] == 0:\n",
    "        zero_target_counter += 1\n",
    "        if zero_target_counter > target_1:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "balance_input = np.delete(shuffled_input, indices_to_remove, axis=0)\n",
    "balance_target = np.delete(shuffled_target, indices_to_remove, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b7854a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted macht 50.0% der Daten aus\n"
     ]
    }
   ],
   "source": [
    "#Target Verteilung checken\n",
    "target_1 = np.sum(balance_target)\n",
    "print(f\"converted macht {target_1/balance_target.shape[0]*100}% der Daten aus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "02fc36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten standardisieren\n",
    "#scaled_input = preprocessing.scale(balance_input) \n",
    "scaled_input = preprocessing.scale(shuffled_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5a7dd9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nochmal Daten mischen, sonst ist die target Verteilung nicht mehr gleichmäßig\n",
    "shuffled_indices = np.arange(scaled_input.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "shuffled_scaled_input = scaled_input[shuffled_indices]\n",
    "#shuffled_balance_target = balance_target[shuffled_indices]\n",
    "shuffled_balance_target = shuffled_target[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "72ef2173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16020236087689713\n",
      "0.16548295454545456\n",
      "0.14123491838183108\n"
     ]
    }
   ],
   "source": [
    "#Daten aufteilen in Trainings- und Testdaten\n",
    "sample_size = shuffled_scaled_input.shape[0]\n",
    "train_size = int(0.8 * sample_size)\n",
    "test_size = int(0.1 * sample_size)\n",
    "validation_size = sample_size - train_size - test_size\n",
    "\n",
    "train_input = shuffled_scaled_input[:train_size]\n",
    "train_target = shuffled_balance_target[:train_size]\n",
    "validation_input = shuffled_scaled_input[train_size:train_size + validation_size]\n",
    "validation_target = shuffled_balance_target[train_size:train_size + validation_size]\n",
    "test_input = shuffled_scaled_input[train_size + validation_size:]\n",
    "test_target = shuffled_balance_target[train_size + validation_size:]\n",
    "\n",
    "#checken ob die Verteilung der Daten gleich ist\n",
    "print(np.sum(train_target)/train_size)\n",
    "print(np.sum(validation_target)/test_size)\n",
    "print(np.sum(test_target)/validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375a6f3",
   "metadata": {},
   "source": [
    "### Schritt 2: Model erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f8208ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_input.shape[1]\n",
    "output_size = 2 #0 und 1\n",
    "hidden_size = 50 #50 ist eine gute Zahl zu starten, kann aber auch höher sein\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7a800024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#wählen wir aktivierungsfunktionen und loss function\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_size, activation='tanh', input_shape=(input_size,)),\n",
    "    tf.keras.layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),\n",
    "    tf.keras.layers.Dense(hidden_size, activation='tanh', input_shape=(input_size,)),\n",
    "    tf.keras.layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),\n",
    "    tf.keras.layers.Dense(hidden_size, activation='tanh', input_shape=(input_size,)),\n",
    "\n",
    "    #wir verwenden hier softmax, weil wir eine Wahrscheinlichkeitsverteilung haben wollen\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5ff708bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000 #es ist eine gute Zahl zu starten, kann aber auch höher sein\n",
    "epochs = 100 #es ist eine gute Zahl zu starten, kann aber auch höher sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1cc4a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setzen frühzeitige stop Funktion ein, um zu verhindern, dass das Modell überfitten\n",
    "#tf.keras.callbacks ist eine Klasse, die eine Callback-Funktion ist, die während des Trainings aufgerufen wird\n",
    "#early_stopping ist eine Callback-Funktion, die das Training stoppt, wenn sich die Validierungsgenauigkeit nicht mehr verbessert\n",
    "#patience ist die Anzahl der Epochen, die gewartet werden, bevor das Training gestoppt wird\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5f916cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1692a7f1be0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trainieren des Modells\n",
    "#batch_size ist die Anzahl der Daten, die in einem Schritt verarbeitet werden\n",
    "model.fit(train_input, \n",
    "          train_target, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_data=(validation_input, validation_target), \n",
    "          verbose=0,\n",
    "          callbacks=[early_stopping]) #verbose=1 zeigt den Fortschritt des Trainings an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aa36c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "#Testen des Modells\n",
    "test_loss, test_accuracy = model.evaluate(test_input, test_target, verbose=0)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb91800",
   "metadata": {},
   "source": [
    "### CNN (Convolutional Neural Network) \n",
    "    ist eine Art von neuronalen Netzwerken, die häufig für Bildverarbeitung und Computer Vision verwendet werden. In diesem Fall haben wir jedoch ein einfaches Feedforward-Netzwerk verwendet, da die Daten nicht bildbasiert sind. CNNs sind in der Regel komplexer und benötigen mehr Daten, um effektiv zu sein. \n",
    "### RNN (Recurrent Neural Network) \n",
    "    ist eine Art von neuronalen Netzwerken, die häufig für sequenzielle Daten verwendet werden. In RNN werden die Ausgaben der vorherigen versteckten Schicht als Eingaben für die nächste Schicht verwendet. RNN s sind in der Regel komplexer und benötigen mehr Daten, um effektiv zu sein.\n",
    "### Non NN (Neural network) \n",
    "    Modelle wie Random Forest, SVM, XGBoost, LightGBM, CatBoost sind auch sehr leistungsfähig und können in vielen Fällen bessere Ergebnisse liefern als neuronale Netzwerke. Diese Modelle sind jedoch nicht so flexibel wie neuronale Netzwerke und benötigen möglicherweise mehr Daten, um effektiv zu sein."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
