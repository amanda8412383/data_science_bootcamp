{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc34d1c",
   "metadata": {},
   "source": [
    "### Schritt 1: Daten Vorbereiten\n",
    "\n",
    "\n",
    "1. Das Datensatz ausgleichen. Die Daten sind unbalanciert, was bedeutet, dass es viel mehr Beispiele für eine Klasse gibt als für die andere. Dies kann zu einem Modell führen, das nicht gut generalisiert. Wir können die Daten ausgleichen, indem wir die Anzahl der Beispiele in jeder Klasse anpassen. In diesem Fall verwenden wir die `RandomOverSampler`-Klasse von `imblearn` um die Daten auszugleichen.\n",
    "2. Die Daten normalisieren. Wir verwenden die `StandardScaler`-Klasse von `sklearn` um die Daten zu normalisieren. Dies ist wichtig, da einige Algorithmen empfindlich auf die Skala der Daten reagieren.\n",
    "3. Die Daten in Trainings- und Testdaten aufteilen. Wir verwenden die `train_test_split`-Funktion von `sklearn` um die Daten in Trainings- und Testdaten aufzuteilen. Dies ist wichtig, um sicherzustellen, dass das Modell auf neuen, unsichtbaren Daten getestet wird.\n",
    "4. Die Daten in Tensoren umwandeln. Wir verwenden die `torch.tensor`-Funktion von `torch` um die Daten in Tensoren umzuwandeln. Dies ist wichtig, da PyTorch mit Tensoren arbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57a0c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\anaconda3\\envs\\bike\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#die Daten einlesen\n",
    "csv = np.loadtxt(\"Audiobooks_data.csv\", delimiter=\",\")\n",
    "input = csv[:, 1:-1] #Spalte 0 ID und letzte Spalte target entfernen\n",
    "target = csv[:, -1] #letzte Spalte target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bccd39d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted macht 15.88327179778472% der Daten aus\n"
     ]
    }
   ],
   "source": [
    "#Target Verteilung checken\n",
    "target_1 = np.sum(target)\n",
    "print(f\"converted macht {target_1/target.shape[0]*100}% der Daten aus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten ausgleichen\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
