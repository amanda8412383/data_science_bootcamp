{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc34d1c",
   "metadata": {},
   "source": [
    "### Schritt 1: Daten Vorbereiten\n",
    "\n",
    "\n",
    "1. Das Datensatz ausgleichen. Die Daten sind unbalanciert, was bedeutet, dass es viel mehr Beispiele für eine Klasse gibt als für die andere. Dies kann zu einem Modell führen, das nicht gut generalisiert. Wir können die Daten ausgleichen, indem wir die Anzahl der Beispiele in jeder Klasse anpassen. In diesem Fall verwenden wir die `RandomOverSampler`-Klasse von `imblearn` um die Daten auszugleichen.\n",
    "2. Die Daten normalisieren. Wir verwenden die `StandardScaler`-Klasse von `sklearn` um die Daten zu normalisieren. Dies ist wichtig, da einige Algorithmen empfindlich auf die Skala der Daten reagieren.\n",
    "3. Die Daten in Trainings- und Testdaten aufteilen. Wir verwenden die `train_test_split`-Funktion von `sklearn` um die Daten in Trainings- und Testdaten aufzuteilen. Dies ist wichtig, um sicherzustellen, dass das Modell auf neuen, unsichtbaren Daten getestet wird.\n",
    "4. Die Daten in Tensoren umwandeln. Wir verwenden die `torch.tensor`-Funktion von `torch` um die Daten in Tensoren umzuwandeln. Dies ist wichtig, da PyTorch mit Tensoren arbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a57a0c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\test\\anaconda3\\envs\\bike\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120b4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#die Daten einlesen\n",
    "csv = np.loadtxt(\"Audiobooks_data.csv\", delimiter=\",\")\n",
    "input = csv[:, 1:-1] #Spalte 0 ID und letzte Spalte target entfernen\n",
    "target = csv[:, -1] #letzte Spalte target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2046dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten mischen\n",
    "shuffled_indices = np.arange(input.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "shuffled_input = input[shuffled_indices]\n",
    "shuffled_target = target[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bccd39d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted macht 15.88327179778472% der Daten aus\n"
     ]
    }
   ],
   "source": [
    "#Target Verteilung checken\n",
    "target_1 = np.sum(shuffled_target)\n",
    "print(f\"converted macht {target_1/shuffled_target.shape[0]*100}% der Daten aus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31eb96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten ausgleichen durch entfernen von 0er Werten\n",
    "zero_target_counter = 0\n",
    "indices_to_remove = []\n",
    "for i in range(shuffled_target.shape[0]):\n",
    "    if shuffled_target[i] == 0:\n",
    "        zero_target_counter += 1\n",
    "        if zero_target_counter > target_1:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "balance_input = np.delete(shuffled_input, indices_to_remove, axis=0)\n",
    "balance_target = np.delete(shuffled_target, indices_to_remove, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7854a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted macht 50.0% der Daten aus\n"
     ]
    }
   ],
   "source": [
    "#Target Verteilung checken\n",
    "target_1 = np.sum(balance_target)\n",
    "print(f\"converted macht {target_1/balance_target.shape[0]*100}% der Daten aus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fc36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten standardisieren\n",
    "scaled_input = preprocessing.scale(balance_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a7dd9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nochmal Daten mischen, sonst ist die target Verteilung nicht mehr gleichmäßig\n",
    "shuffled_indices = np.arange(scaled_input.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "shuffled_scaled_input = scaled_input[shuffled_indices]\n",
    "shuffled_balance_target = balance_target[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72ef2173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49958088851634536\n",
      "0.49440715883668906\n",
      "0.5089285714285714\n"
     ]
    }
   ],
   "source": [
    "#Daten aufteilen in Trainings- und Testdaten\n",
    "sample_size = shuffled_scaled_input.shape[0]\n",
    "train_size = int(0.8 * sample_size)\n",
    "test_size = int(0.1 * sample_size)\n",
    "validation_size = sample_size - train_size - test_size\n",
    "\n",
    "train_input = shuffled_scaled_input[:train_size]\n",
    "train_target = shuffled_balance_target[:train_size]\n",
    "validation_input = shuffled_scaled_input[train_size:train_size + validation_size]\n",
    "validation_target = shuffled_balance_target[train_size:train_size + validation_size]\n",
    "test_input = shuffled_scaled_input[train_size + validation_size:]\n",
    "test_target = shuffled_balance_target[train_size + validation_size:]\n",
    "\n",
    "#checken ob die Verteilung der Daten gleich ist\n",
    "print(np.sum(train_target)/train_size)\n",
    "print(np.sum(validation_target)/test_size)\n",
    "print(np.sum(test_target)/validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375a6f3",
   "metadata": {},
   "source": [
    "### Schritt 2: Model erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8208ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = train_input.shape[1]\n",
    "output_size = 2 #0 und 1\n",
    "hidden_size = 50 #50 ist eine gute Zahl zu starten, kann aber auch höher sein\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
